\chapter{Context \& State of the Art}

\section{Credit Scoring Fundamentals}

Credit scoring represents one of the most consequential applications of statistical analysis in the financial industry. At its core, credit scoring is the process by which lenders and financial institutions evaluate the creditworthiness of potential borrowers. This evaluation determines whether to extend credit, what terms to offer, and what interest rates to charge. The objectivity provided by algorithmic approaches has largely replaced the subjective judgment that once characterized lending decisions.

Historically, credit assessment relied on what bankers called the ``Five Cs of Credit'': Character (the borrower's reputation and track record), Capacity (the ability to repay based on income), Capital (the borrower's existing assets), Collateral (assets pledged against the loan), and Conditions (the purpose of the loan and economic environment). In the algorithmic era, these conceptual categories translate into features in a dataset, numerical and categorical variables that a model can learn to associate with default risk.

\section{The Class Imbalance Challenge}

One of the fundamental challenges in credit risk modeling is the inherent imbalance between classes. In any representative portfolio, the majority of borrowers will repay their loans as agreed, while only a small fraction will default. A typical dataset might contain ninety-five percent good payers and only five percent defaulters. This imbalance creates a subtle but serious problem for machine learning algorithms.

When trained on imbalanced data, most classifiers will optimize for overall accuracy by learning to predict the majority class almost exclusively. A trivial model that predicts ``good payer'' for every application would achieve ninety-five percent accuracy yet provide absolutely no value for risk management. The model would fail to identify the very cases that matter mostâ€”the potential defaulters who could cause financial losses.

Various techniques exist to address this challenge. Some approaches modify the training data through oversampling the minority class or undersampling the majority class. Others adjust the loss function to penalize misclassification of the minority class more heavily. The choice of technique depends on the specific dataset and business requirements.

\section{The Rise of MLOps}

A pivotal moment in the machine learning industry occurred with the publication of Google's paper titled ``Hidden Technical Debt in Machine Learning Systems.'' This research demonstrated that in production machine learning systems, the actual machine learning code represents only a small fraction of the overall codebase. The vast majority consists of infrastructure for data collection, feature extraction, configuration management, serving, and monitoring.

This insight gave rise to MLOps, a discipline that applies DevOps principles to machine learning systems. Just as DevOps unified software development and IT operations to enable continuous delivery of traditional applications, MLOps aims to unify machine learning development with production operations. The goal is to reduce the friction between data scientists who build models and the engineering teams responsible for deploying and maintaining them.

Key components of an MLOps practice include data versioning, which treats datasets with the same rigor as source code. When data changes, the system should track what changed, when, and why. Experiment tracking provides similar rigor for model training, recording hyperparameters, metrics, and artifacts for every training run. A model registry formalizes the lifecycle of models through stages such as development, staging, and production. Continuous training extends continuous integration to include automated retraining when data drift is detected or on a scheduled basis.

\section{Generative AI in Financial Services}

The emergence of Large Language Models such as GPT-5 and Google Gemini 3 Pro has opened new possibilities for human-computer interaction. These models can generate coherent, contextually appropriate text that was previously possible only through human effort. In financial services, this capability enables a shift from purely algorithmic outputs to natural language explanations.

Rather than presenting users with a numerical risk score or a binary approval decision, systems can now generate personalized explanations in natural language. A rejection is no longer simply ``Application Denied'' but can become a constructive explanation of the factors that influenced the decision and suggestions for how the applicant might improve their profile. This transformation from punitive rejection to constructive advisory represents a significant improvement in user experience.

It is important to note that in this application, the generative AI component does not participate in the risk assessment itself. The credit decision remains the domain of the traditional machine learning model, where reliability and consistency are paramount. The language model serves as an interpretation layer, translating the structured output of the scoring system into natural language that users can understand and act upon.
