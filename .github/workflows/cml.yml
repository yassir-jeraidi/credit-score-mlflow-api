name: CML - Continuous Machine Learning

on:
  push:
    branches: [main, dev]
    paths:
      - "ml/**"
      - "requirements.txt"
      - ".github/workflows/cml.yml"
  pull_request:
    branches: [main, dev]
    paths:
      - "ml/**"
      - "requirements.txt"
  workflow_dispatch:
    inputs:
      n_samples:
        description: "Number of training samples"
        required: true
        default: "1000"
        type: string
      full_training:
        description: "Run full training (50k samples)"
        required: false
        default: false
        type: boolean

env:
  PYTHON_VERSION: "3.11"
  AWS_DEFAULT_REGION: eu-west-3

jobs:
  # ===========================================================================
  # Train Verification (Small Dataset)
  # ===========================================================================
  train-verification:
    name: Training Verification
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: write
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-ml-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-ml-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Configure DVC
        run: |
          dvc remote modify s3remote --local access_key_id ${{ secrets.AWS_ACCESS_KEY_ID }}
          dvc remote modify s3remote --local secret_access_key ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}

      - name: Pull data from DVC (if available)
        run: |
          dvc pull || echo "DVC pull failed or no data cached, will generate fresh data"
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        continue-on-error: true

      - name: Determine sample size
        id: samples
        run: |
          if [ "${{ github.event.inputs.full_training }}" == "true" ]; then
            echo "n_samples=50000" >> $GITHUB_OUTPUT
          elif [ -n "${{ github.event.inputs.n_samples }}" ]; then
            echo "n_samples=${{ github.event.inputs.n_samples }}" >> $GITHUB_OUTPUT
          else
            echo "n_samples=1000" >> $GITHUB_OUTPUT
          fi

      - name: Generate test data
        run: |
          python -m ml.data_generator --n-samples ${{ steps.samples.outputs.n_samples }} --seed 42
          echo "Generated ${{ steps.samples.outputs.n_samples }} samples for training verification"

      - name: Run training verification
        id: train
        run: |
          # Create a local MLflow tracking directory
          mkdir -p mlruns

          # Run training with local MLflow
          python -c "
          import sys
          sys.path.insert(0, '.')
          from ml.train import train_model, create_model_pipeline, evaluate_model
          from ml.data_generator import load_from_csv, split_data
          from ml.config import TEST_SIZE, DEFAULT_MODEL_PARAMS
          import mlflow
          import json

          # Use local file-based MLflow tracking
          mlflow.set_tracking_uri('file:./mlruns')

          # Load data
          df = load_from_csv()
          X_train, X_test, y_train, y_test = split_data(df, test_size=TEST_SIZE)

          print(f'Training data shape: {X_train.shape}')
          print(f'Test data shape: {X_test.shape}')

          # Create and train model
          model = create_model_pipeline(DEFAULT_MODEL_PARAMS)
          model.fit(X_train, y_train)

          # Evaluate
          metrics = evaluate_model(model, X_test, y_test)

          # Print metrics
          print('\\n=== Model Metrics ===')
          for name, value in metrics.items():
              print(f'{name}: {value:.4f}')

          # Save metrics to file for CML report
          with open('metrics.json', 'w') as f:
              json.dump(metrics, f, indent=2)

          print('\\n‚úÖ Training verification completed successfully!')
          "
        env:
          PYTHONPATH: .

      - name: Validate model metrics
        run: |
          python -c "
          import json

          with open('metrics.json', 'r') as f:
              metrics = json.load(f)

          # Define minimum acceptable thresholds
          thresholds = {
              'accuracy': 0.70,
              'precision': 0.65,
              'recall': 0.65,
              'f1_score': 0.65,
              'roc_auc': 0.75
          }

          failed = []
          for metric, threshold in thresholds.items():
              if metrics.get(metric, 0) < threshold:
                  failed.append(f'{metric}: {metrics.get(metric, 0):.4f} < {threshold}')

          if failed:
              print('‚ùå Model did not meet minimum thresholds:')
              for f in failed:
                  print(f'  - {f}')
              # Don't fail for small training samples - just warn
              print('\\n‚ö†Ô∏è Note: Low metrics may be due to small sample size for verification')
          else:
              print('‚úÖ All metrics meet minimum thresholds!')
          "

      - name: Setup CML
        if: github.event_name == 'pull_request'
        uses: iterative/setup-cml@v2

      - name: Generate CML Report
        if: github.event_name == 'pull_request'
        env:
          REPO_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # Create CML report
          echo "## ü§ñ ML Training Verification Report" >> report.md
          echo "" >> report.md
          echo "**Samples:** ${{ steps.samples.outputs.n_samples }}" >> report.md
          echo "**Branch:** ${{ github.head_ref }}" >> report.md
          echo "**Commit:** ${{ github.sha }}" >> report.md
          echo "" >> report.md

          echo "### üìä Model Metrics" >> report.md
          echo "" >> report.md
          echo "| Metric | Value |" >> report.md
          echo "|--------|-------|" >> report.md

          python -c "
          import json
          with open('metrics.json', 'r') as f:
              metrics = json.load(f)
          for name, value in metrics.items():
              print(f'| {name} | {value:.4f} |')
          " >> report.md

          echo "" >> report.md
          echo "### ‚úÖ Training Status" >> report.md
          echo "" >> report.md
          echo "Training verification completed successfully. The model training pipeline is working correctly." >> report.md
          echo "" >> report.md
          echo "---" >> report.md
          echo "*Generated by CML on $(date)*" >> report.md

          # Post comment to PR
          cml comment create report.md

      - name: Upload metrics artifact
        uses: actions/upload-artifact@v4
        with:
          name: training-metrics
          path: |
            metrics.json
            mlruns/

  # ===========================================================================
  # Data Validation
  # ===========================================================================
  data-validation:
    name: Data Validation
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas numpy

      - name: Validate data generation
        run: |
          python -c "
          import sys
          sys.path.insert(0, '.')
          from ml.data_generator import generate_credit_data
          from ml.config import NUMERICAL_FEATURES, CATEGORICAL_FEATURES

          # Generate small sample
          df = generate_credit_data(n_samples=100, random_state=42)

          # Validate schema
          expected_columns = NUMERICAL_FEATURES + CATEGORICAL_FEATURES + ['target']
          missing = set(expected_columns) - set(df.columns)
          extra = set(df.columns) - set(expected_columns)

          if missing:
              print(f'‚ùå Missing columns: {missing}')
              sys.exit(1)
          if extra:
              print(f'‚ö†Ô∏è Extra columns: {extra}')

          # Validate data types
          print('‚úÖ Schema validation passed')

          # Validate no nulls
          if df.isnull().any().any():
              print('‚ùå Found null values')
              sys.exit(1)
          print('‚úÖ No null values')

          # Validate target distribution
          target_dist = df['target'].value_counts(normalize=True)
          print(f'Target distribution: {target_dist.to_dict()}')

          # Check for reasonable class balance (not extremely imbalanced)
          min_class_ratio = target_dist.min()
          if min_class_ratio < 0.1:
              print(f'‚ö†Ô∏è Warning: Class imbalance detected (min class: {min_class_ratio:.2%})')
          else:
              print('‚úÖ Reasonable class balance')

          print('\\n‚úÖ All data validations passed!')
          "
        env:
          PYTHONPATH: .

  # ===========================================================================
  # Model Code Quality
  # ===========================================================================
  ml-code-quality:
    name: ML Code Quality
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install flake8 black isort

      - name: Check ML code formatting
        run: |
          black --check --diff ml/
          isort --check-only --diff ml/

      - name: Lint ML code
        run: |
          flake8 ml/ --max-line-length=100 --ignore=E501,W503
