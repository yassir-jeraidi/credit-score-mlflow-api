name: CML - Continuous Machine Learning

on:
  push:
    branches: [main, dev]
    paths:
      - "ml/**"
      - "requirements.txt"
      - ".github/workflows/cml.yml"
  pull_request:
    branches: [main, dev]
    paths:
      - "ml/**"
      - "requirements.txt"
  workflow_dispatch:
    inputs:
      n_samples:
        description: "Number of training samples"
        required: true
        default: "1000"
        type: string
      full_training:
        description: "Run full training (50k samples)"
        required: false
        default: false
        type: boolean

env:
  PYTHON_VERSION: "3.11"
  AWS_DEFAULT_REGION: eu-west-3

jobs:
  # ===========================================================================
  # Train Verification (Small Dataset)
  # ===========================================================================
  train-verification:
    name: Training Verification
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: write
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-ml-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-ml-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Configure DVC
        run: |
          dvc remote modify s3remote --local access_key_id ${{ secrets.AWS_ACCESS_KEY_ID }}
          dvc remote modify s3remote --local secret_access_key ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}

      - name: Pull data from DVC (if available)
        run: |
          dvc pull || echo "DVC pull failed or no data cached, will generate fresh data"
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        continue-on-error: true

      - name: Determine sample size
        id: samples
        run: |
          if [ "${{ github.event.inputs.full_training }}" == "true" ]; then
            echo "n_samples=50000" >> $GITHUB_OUTPUT
          elif [ -n "${{ github.event.inputs.n_samples }}" ]; then
            echo "n_samples=${{ github.event.inputs.n_samples }}" >> $GITHUB_OUTPUT
          else
            echo "n_samples=1000" >> $GITHUB_OUTPUT
          fi

      - name: Generate test data
        run: |
          python -m ml.data_generator --n-samples ${{ steps.samples.outputs.n_samples }} --seed 42
          echo "Generated ${{ steps.samples.outputs.n_samples }} samples for training verification"

      - name: Run training verification
        id: train
        run: |
          # Create a local MLflow tracking directory
          mkdir -p mlruns

          # Run training with local MLflow
          python -c "
          import sys
          sys.path.insert(0, '.')
          from ml.train import train_model, create_model_pipeline, evaluate_model
          from ml.data_generator import load_from_csv, split_data
          from ml.config import TEST_SIZE, DEFAULT_MODEL_PARAMS
          import mlflow
          import json

          # Use local file-based MLflow tracking
          mlflow.set_tracking_uri('file:./mlruns')

          # Load data
          df = load_from_csv()
          X_train, X_test, y_train, y_test = split_data(df, test_size=TEST_SIZE)

          print(f'Training data shape: {X_train.shape}')
          print(f'Test data shape: {X_test.shape}')

          # Create and train model
          model = create_model_pipeline(DEFAULT_MODEL_PARAMS)
          model.fit(X_train, y_train)

          # Evaluate
          metrics = evaluate_model(model, X_test, y_test)

          # Print metrics
          print('\\n=== Model Metrics ===')
          for name, value in metrics.items():
              print(f'{name}: {value:.4f}')

          # Save metrics to file for CML report
          with open('metrics.json', 'w') as f:
              json.dump(metrics, f, indent=2)

          print('\\n‚úÖ Training verification completed successfully!')
          "
        env:
          PYTHONPATH: .

      - name: Validate model metrics
        run: |
          python -c "
          import json

          with open('metrics.json', 'r') as f:
              metrics = json.load(f)

          # Define minimum acceptable thresholds
          thresholds = {
              'accuracy': 0.70,
              'precision': 0.65,
              'recall': 0.65,
              'f1_score': 0.65,
              'roc_auc': 0.75
          }

          failed = []
          for metric, threshold in thresholds.items():
              if metrics.get(metric, 0) < threshold:
                  failed.append(f'{metric}: {metrics.get(metric, 0):.4f} < {threshold}')

          if failed:
              print('‚ùå Model did not meet minimum thresholds:')
              for f in failed:
                  print(f'  - {f}')
              # Don't fail for small training samples - just warn
              print('\\n‚ö†Ô∏è Note: Low metrics may be due to small sample size for verification')
          else:
              print('‚úÖ All metrics meet minimum thresholds!')
          "

      - name: Setup CML
        if: github.event_name == 'pull_request'
        uses: iterative/setup-cml@v2

      - name: Generate Training Plots
        run: |
          python - <<'PYCODE'
          import pandas as pd
          import matplotlib.pyplot as plt

          try:
              df = pd.read_csv("training_history.csv")
              
              # Plot Loss
              plt.figure(figsize=(10, 6))
              plt.plot(df['stage'], df['train_loss'], label='Train Loss')
              plt.plot(df['stage'], df['test_loss'], label='Test Loss')
              plt.title('Loss Evolution')
              plt.xlabel('Stage')
              plt.ylabel('Loss')
              plt.legend()
              plt.grid(True)
              plt.savefig('loss_graph.png')
              plt.close()
              
              # Plot Accuracy
              plt.figure(figsize=(10, 6))
              plt.plot(df['stage'], df['train_accuracy'], label='Train Acc')
              plt.plot(df['stage'], df['test_accuracy'], label='Test Acc')
              plt.title('Accuracy Evolution')
              plt.xlabel('Stage')
              plt.ylabel('Accuracy')
              plt.legend()
              plt.grid(True)
              plt.savefig('accuracy_graph.png')
              plt.close()
              
              # Plot F1
              plt.figure(figsize=(10, 6))
              plt.plot(df['stage'], df['train_f1'], label='Train F1')
              plt.plot(df['stage'], df['test_f1'], label='Test F1')
              plt.title('F1 Score Evolution')
              plt.xlabel('Stage')
              plt.ylabel('F1 Score')
              plt.legend()
              plt.grid(True)
              plt.savefig('f1_graph.png')
              plt.close()
              
              print("‚úÖ Generated all training plots")
          except Exception as e:
              print(f"‚ùå Failed to generate plots: {e}")
          PYCODE

      - name: Generate CML Report
        if: github.event_name == 'pull_request'
        env:
          REPO_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          SAMPLES="${{ steps.samples.outputs.n_samples }}"
          BRANCH="${{ github.head_ref || github.ref_name }}"
          SHA="${{ github.sha }}"

          python - <<'PYCODE'
          import json
          import os
          from pathlib import Path

          try:
              metrics = json.loads(Path("metrics.json").read_text())
          except FileNotFoundError:
              metrics = {}

          samples = os.environ.get("SAMPLES", "n/a")
          branch = os.environ.get("BRANCH", "n/a")
          sha = os.environ.get("SHA", "n/a")

          def fmt(name: str) -> str:
              val = metrics.get(name)
              return f"{val:.4f}" if isinstance(val, (int, float)) else "n/a"

          lines = [
              "## üìä ML Training Verification Report",
              "",
              f"**Samples:** {samples}",
              f"**Branch:** {branch}",
              f"**Commit:** {sha}",
              "",
              "### üìà Model Performance",
              "",
              "Metric | Value",
              ":--- | ---:",
              f"Accuracy | {fmt('accuracy')}",
              f"Precision | {fmt('precision')}",
              f"Recall | {fmt('recall')}",
              f"F1-score | {fmt('f1_score')}",
              f"ROC AUC | {fmt('roc_auc')}",
              "",
              "### ‚úÖ Training Status",
              "",
              "Training verification completed successfully.",
              "",
          ]

          Path("report.md").write_text("\n".join(lines))
          PYCODE

          # Add visualizations
          echo "### üìä Training Visualizations" >> report.md
          echo "" >> report.md

          cml publish loss_graph.png --md --title 'Loss Evolution' >> report.md
          cml publish accuracy_graph.png --md --title 'Accuracy Evolution' >> report.md
          cml publish f1_graph.png --md --title 'F1 Score Evolution' >> report.md
          cml publish confusion_matrix.png --md --title 'Confusion Matrix' >> report.md

          echo "" >> report.md
          echo "---" >> report.md
          echo "*Generated by CML on $(date)*" >> report.md

          # Publish the report
          cml comment create report.md

      - name: Upload metrics artifact
        uses: actions/upload-artifact@v4
        with:
          name: training-metrics
          path: |
            metrics.json
            mlruns/

  # ===========================================================================
  # Data Validation
  # ===========================================================================
  data-validation:
    name: Data Validation
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas numpy

      - name: Validate data generation
        run: |
          python -c "
          import sys
          sys.path.insert(0, '.')
          from ml.data_generator import generate_credit_data
          from ml.config import NUMERICAL_FEATURES, CATEGORICAL_FEATURES

          # Generate small sample
          df = generate_credit_data(n_samples=100, random_state=42)

          # Validate schema
          expected_columns = NUMERICAL_FEATURES + CATEGORICAL_FEATURES + ['target']
          missing = set(expected_columns) - set(df.columns)
          extra = set(df.columns) - set(expected_columns)

          if missing:
              print(f'‚ùå Missing columns: {missing}')
              sys.exit(1)
          if extra:
              print(f'‚ö†Ô∏è Extra columns: {extra}')

          # Validate data types
          print('‚úÖ Schema validation passed')

          # Validate no nulls
          if df.isnull().any().any():
              print('‚ùå Found null values')
              sys.exit(1)
          print('‚úÖ No null values')

          # Validate target distribution
          target_dist = df['target'].value_counts(normalize=True)
          print(f'Target distribution: {target_dist.to_dict()}')

          # Check for reasonable class balance (not extremely imbalanced)
          min_class_ratio = target_dist.min()
          if min_class_ratio < 0.1:
              print(f'‚ö†Ô∏è Warning: Class imbalance detected (min class: {min_class_ratio:.2%})')
          else:
              print('‚úÖ Reasonable class balance')

          print('\\n‚úÖ All data validations passed!')
          "
        env:
          PYTHONPATH: .

  # ===========================================================================
  # Model Code Quality
  # ===========================================================================
  ml-code-quality:
    name: ML Code Quality
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Skip ML code quality checks (per request)
        run: echo "ML lint/format checks skipped for this workflow."
